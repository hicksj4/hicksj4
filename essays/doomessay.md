---
layout: essay
type: essay
title: "An Analysis of Game Development Evolution : From Doom (1993) to Quake (1996)"
# All dates must be YYYY-MM-DD format!
date: 2023-12-17
published: true
labels:
  - Game Development
  - 3D Graphics
---

An Analysis of Game Development Evolution : From Doom (1993) to Quake (1996)

Overview : id Software’s games Doom (1993) and Quake (1996) pushed the technological boundaries on video games, particularly the first-person-shooter genre, one of the most popular modern day game genres (ie. Counter Strike, Call of Duty, Valorant, Overwatch, Halo, etc.). In this paper I want to present the history and analysis on the techniques that made these games so influential in the game development world, beginning with Doom.

Problem #1 : Pushing the Boundaries of Modern Computer Game Graphics
Solution #1 : Graphics Rendering Utilizing Binary Space Partitioning Trees 
In 1992, id Software released their second game, Wolfenstein 3D, which received critical acclaim and was even termed “the grandfather of 3D shooters”. However, it wasn’t until 1993 when id Software released Doom (1993) which sent a wave through the gaming industry as a whole due to a number of factors such as its technical boundary pushing, replayability, and immersive world. In this section I want to highlight and analyze one of the main technical concepts that was utilized in Doom’s engine that made it so influential to game development as a whole. To preface, in the late 1990’s, retail consumers' computers were weak, especially compared to modern day technology, and not only were they weak; they were expensive (upwards of $2000). Thus in game development, optimizations had to be made to ensure that every player that had at least an average home computer could play the game, especially if the goal is to make a more immersive and realistic world. Prior to Doom, Wolfenstein utilized a technique in graphic rendering called ray casting. Ray casting was a process of rendering the graphics of the game from the eye of the observer/the player's perspective, casting a “ray” from left to right and utilizing trigonometric functions to determine the size of the rendered object.

However, ray casting was costly to computer systems at the time, as to achieve real-time gameplay, the ray casting algorithm had to be performed at real-time as the player moved around. Hence, there was a heavy load on the CPU as for every vertical line/x-value that had to be rendered, the CPU had to constantly calculate the trigonometric values to determine the distance between the object (generally a wall) and the player. A far more in depth explanation of ray casting can be found here. 
	During the development of Doom, there was a particular level that the ray casting algorithm was struggling with, despite optimizations made by the team. At this same time, Nintendo had contracted id Software to port Wolfenstein 3D to the Super Nintendo, which the team had eventually realized that the Super Nintendo was incapable of running Wolfenstein. In search of a more efficient rendering technique, John Carmack, lead programmer at id Software, discovered and implemented a technique which greatly improved graphics rendering speed utilizing a concept called ‘Binary Space Partitioning’.
	Before we go on, you may wonder why we use these techniques such as Binary Space Partitioning and ray casting? The main purpose is to adhere to the concept of “Hidden-surface determination”, which I will talk about more later. The concept is basically that we only render what is visible to the player’s field of vision, and we do not render what the player can’t see. By only processing and rendering what the viewer can see, the computational load is significantly reduced. This is especially important in complex scenes with a lot of geometry or in real-time applications like video games, where maintaining a high frame rate is crucial.
Algorithm and Implementation 
Binary Space Partitioning (BSP)
Data Structure : Binary Tree
The main core data structure of the BSP concept is a binary tree. Binary Space Partitioning is a method for recursively dividing a space into convex sets by using hyperplanes. What this means in the context of Doom is we take the map data; the map data being essentially a big 2D plane that is separated by connected vertices, and you begin by picking one polygon in your scene and making the plane in which the polygon lies your partitioning plane. That one polygon ends up as the root node in your tree (In Doom’s case this is the entire map of the level). The remaining polygons in your scene will be on one side or the other of your root partitioning plane (So there are rooms to the left and right of the first polygon’s partition plane). The polygons on the “forward” side or in the “forward” half-space of your plane end up in the left subtree of your root node, while the polygons on the “back” side or in the “back” half-space of your plane end up in the right subtree. You then repeat this process recursively, picking a polygon from your left and right subtrees to be the new partitioning planes for their respective half-spaces, which generates further half-spaces and further sub-trees. You stop when you run out of polygons.

Time Complexity
Constructing the Tree (O(n log n)) :
In constructing the tree, we must partition the set of elements approximately in half, this leads to a tree depth of about log n, similar to a balanced binary tree. Next, for each level of the tree, when we partition the space we must determine which side of the plane each node falls on, either front or back (of the line partition). This would lead to about O(n) work at each level for the nodes. Combining both of these would lead to an O(n log n) time complexity in the average case. In the worst case scenario this run time could probably be O(n^2) if the placement of polygons/vertices were done in a very overlapped way (Thus adding more amounts of nodes needed to be created), but for our sake, we’ll stick to the Doom 2D map planes which were relatively simple for the most part.
Rendering (AKA Traversing the Tree) (O(n)) : 
Traversing a binary tree is typically O(n), however in our case for the BSP tree it was usually much lower than O(n) and this is the reason why it was so influential to the game development world. Because we know where the player's field of vision is (the concept is similar to ray casting, we have the observer's eye seeing a 90 degree angle sweeping forward), we can at all times determine the location of the player in terms of what node they are in. Thus, by knowing what node the player is in in the tree, we can optimize how we render the scene in such a way that we ONLY render what is visible to the player in that scene. This is very efficient because we don’t have to render anything the player does not see, and this is why splitting the BSP tree into a front/back structure also makes it more efficient.
One of the significant reasons why this was so efficient is that the BSP tree was calculated before the game. So while the player loads into the game, the BSP tree is constructed, thus saving the CPU from having to render the map in real time like ray casting does. This was possible because the map data for levels did not change, they were static elements that were fixed, so there was no need to add or remove nodes from a tree at all.

Transition to Quake (1996) :
id Software’s successor to Doom was Quake. Released in 1996, Quake built upon the technology of its predecessor. Particularly in innovating an even more immersive game world, utilizing real-time 3D rendering and the ability to play online multiplayer.

Problem #2 : Pushing the Boundaries of Modern Computer Game Graphics (2.0)
Solution #2 : Graphics Rendering Utilizing Visible-Surface Determination & PVS
To improve from Doom’s graphics, id Software still sought to create a game that was truly ‘3D’ and not ‘2.5D’ as Doom was. Thanks to John Carmacks utilization of BSP trees for Doom, id was able to build upon this concept to bring about an efficient and effective way of rendering 3D spaces that would allow Quake to have what is referred to as the true ‘six degrees of freedom’, or more simply, in Quake we could now look up and down! To preface, it’s worth noting that the average computer at this time did not have dedicated graphics cards (GPU) like today. A majority of the average personal computer’s calculations was done through the CPU, hence the need to optimize as best the game developers could. CPU’s in this era had around 50-75 MHz clock speed power, my current laptop (a mid-high range gaming laptop), can run up to 5GHz (5000 MHz, about 100x more powerful, and that's not accounting the GPU as well).
	To optimize 3D real-time rendering was no simple task, id built upon the same concept developed in Doom, BSP trees. We split the map data using the concepts taken from BSP and place them into a binary tree structure, however the difference between Doom and Quake is that now we refer to the edges between the nodes as ‘portals’. Put more simply, the nodes of this BSP tree graph are the rooms, and the edges are the doorways between these rooms.
Algorithm and Implementation	
Potentially Visible Set Precalculation (PVS)
Data Structure : Binary Tree
Quake utilized a concept called Potential Visible Set Precalculation (PVS) which was key in optimizing rendering speed and efficiency. We build upon the same BSP tree like we did in Doom, but here we also use PVS calculations, which will store data in the leaves of the BSP tree to determine visibility between leaves. How PVS works is it would focus on visibility between portals. To give an simple example, let’s focus on a portion of our BSP tree that is linear (ie. a leaf of the tree, connected to one singular leaf). Think of a long and completely straight hallway space, starting with the room you are in, the hallway (which will be split into two), and the room at the end of the hallway. The PVS algorithm will calculate if you can see from your room’s portal, the other portals.
Because our scenario is a completely straight hallway, yes, you can see all other portals, you can see all of the hallway from your portal, and you can see into the room at the end of the hallway. But what would happen if say the hallway were shaped differently, such as the hallway bends at a sharp angle at its midway point, thus preventing you from seeing into the room at the end of the hall from your room? This is what PVS seeks to calculate in order to be efficient at rendering.

Above is a visual representation of the map rendering. Notice how from our room (leaf 1), there is no possible way to visualize edge 3 (and thus leaf 4), therefore PVS will stop and determine from leaf 1 (your room), you can only “see” leaf 2 (hall 1), and leaf 3 (hall 2). This data is stored into leaf 1 as what is the potentially visible set.
This process repeats recursively for every node in the BSP tree, utilizing a DFS, where each sequence terminates when a leaf is determined to be invisible from the current one we are checking (ie. our above example terminates when leaf 4 is determined to not be visible from leaf 1). This process is important in the same vein of how we described in the Doom game engine, we want to only render what is visible to the player at any given time/location, to optimize rendering processes.
Time Complexity
Constructing the BSP tree is done in the same way previously described. Here I’ll just focus on the PVS calculations for this section.
PVS Precalculation (O(n^2)) (Most likely greater than n^2) : 
Unfortunately in the PVS calculation, we can reach time complexity over n^2 and greater due to the visibility calculations that must be performed between each leaf. n^2 being the result as in the worst case scenario, each leaf node (n) has to be checked for visibility against every other leaf (n^2). This however is a very unlikely result, as we can see in our example above, leaf 1 had its PVS calculation stop at leaf 4, therefore we didn’t need to keep calculating visibility on to leaf 5, leaf 6, and so on.

Above is a deeper visual example of how the PVS calculation works. In this example, we are calculating leaf 1’s PVS. Notice how edge 3 is a triangular shape. This is because PVS works by using a plane (called a separator) which extends from each side of our edges to determine visibility. Without visualization this is somewhat complicated to explain, but first we check edge 1’s visibility to edge 2, edge 1 can see all of edge 2 therefore all of edge 2’s portal is retained. However, when we check edge 2 to edge 3, using our separators we determine that only about half of edge 3 was visible from edge 2 (Thus it gets cut in half, resulting in that triangle shape). This doesn’t matter though because this still means that edge 3 is still visible from edge 1. If none of edge 3 was left after the edge 2 separator check (ie. edge 3 was completely non-visible from edge 2), we can stop there as it is determined that edge 3 is not visible at all from edge 1.
The important thing to note about all this though, is that even though the PVS calculation is intensive and can get highly complex quickly if the map data is very complex, this PVS data is pre-processed before the game loads and not during the real-time gameplay. This is similar to how in Doom we precalculated the Doom BSP before gameplay, to optimize performance. For both of these games, the map data being a static element allows us to perform these optimizations in rendering, as the BSP tree, and in turn the PVS data, will always be static.
Below is the visibility data calculated for Quake’s Level 1 map.

Conclusion
The innovations that were built and utilized from Doom to Quake were very influential to the gaming world, especially for that era, and is still fascinating to research today. I wanted to explore this topic because I felt that the reasons why these technologies were used represented a core concept to our course, particularly in using algorithms to be more efficient and effective in the implementations of programs/code, and the connection between the data structures and these algorithms which made it so effective.
